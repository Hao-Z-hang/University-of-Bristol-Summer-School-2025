{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1e5f16a-489e-44cf-8f0d-ad28fa07d28a",
   "metadata": {},
   "source": [
    "# Session: Urban Analytics through Social Media"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac28462-bc5f-4a13-82a7-137472bca3da",
   "metadata": {},
   "source": [
    "In this session, we will learn:\n",
    "- How to retrieve and analyse unstructured geo-text.\n",
    "- How to apply WordCloud to visualise text information.\n",
    "- How to extract the hidden topics from large volumes     of text by applying Latent Dirichlet Allocation(LDA)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64acacf6-cd0e-46ca-9a72-51b38081af39",
   "metadata": {},
   "source": [
    "Required packages include:\n",
    "- [pandas](https://pandas.pydata.org/docs/getting_started/install.html)\n",
    "- [wordcloud](https://pypi.org/project/wordcloud/)\n",
    "- [gensim](https://pypi.org/project/gensim/)\n",
    "- [nltk](https://www.nltk.org/install.html)\n",
    "- [folium](https://pypi.org/project/folium/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b1cfd3-d456-4bc6-8c49-127fa9c4219b",
   "metadata": {},
   "source": [
    "Mostly, you can use the commond like `pip install pandas` to install the packages. However, the environment has been configured in advance, so you can skip this step directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be2492c-12e1-4cf9-a91b-7421dd595644",
   "metadata": {},
   "source": [
    "## 1 Retrieve data from location-based social media"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5cf09a-cde9-43e0-8f8f-c15cdedacd36",
   "metadata": {},
   "source": [
    "We can retrieve information from many location-based social media, such as Foursqure, Tweeter, Google Places, Yelp, and so on. The key technique of doing so is through API (Application Programming Interface), which helps to bridge your program with the database these services are using/collecting. In this session, you don't need to retrieve the data from the beginning (but API is an important tool you will learn from MSc in GDSSA). We have already shared the cleaned data in the forder. In the next steps, let's load the data directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71bc2ea-0045-4006-afea-50dded991c71",
   "metadata": {},
   "source": [
    "### 1.1 Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8303608c-7185-460f-8733-41f6db0e4500",
   "metadata": {},
   "source": [
    "In this session, we mainly use two datasets: Foursquare and Yelp. You can choose either one for your experiment. If you downloaded `Foursquare_response.csv` to your local directory, you should be able to load the data by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98eca963-1d65-4ebb-bc71-64c20521bb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd9a826-033a-46c5-9484-67cd2a62babf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Read the file\n",
    "examples = pd.read_csv('./Foursquare_response.csv')# make sure the directory is correct in your case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54f856d-4134-4a3d-9d83-8917a42b0aed",
   "metadata": {},
   "source": [
    "There are many operations in Python to view/describe your data. For example, you can use `.head()` to check the first several rows of your dataframe, `.info()` to have a list of information about your dataframe (you are suggested to always run it first after your data is loaded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d01bb842-c029-4481-aa02-b32e192a6b23",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'examples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# How does it look?\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mexamples\u001b[49m\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'examples' is not defined"
     ]
    }
   ],
   "source": [
    "# How does it look?\n",
    "examples.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6422fa-f9e1-455b-8b7a-fb4b1f5d1dfa",
   "metadata": {},
   "source": [
    "Foursquare data includes geographic information of local venues in a city (Bristol, as an example, in this practical), such as latitude, longitude, address, category, and name of the venue. Geographic coordinates (latitude + longitude) are very important variables in urban analysis! \n",
    "\n",
    "We can use geographic coordinates to visualise and map locations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98cada9-8c31-4edb-af18-cc7b92ece1f5",
   "metadata": {},
   "source": [
    "## 2 Mapping the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b3d6eb-8366-4022-a81f-3f6f5c5fddf8",
   "metadata": {},
   "source": [
    "There is a Python library called `Folium` that makes it possible to visualise geographic data in Python on an interactive Leaflet map (Leaflet is a Javascript library to build interactive maps. `Folium` is a Python library that encapsulates functions of Leaflet allowing its re-use in Python). \n",
    "\n",
    "Note that there are other ways to visualise geographic data, which will be taught at MSc in GDSSA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82c84fa-02eb-4c59-8c9f-fa7a9db3a008",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "# Create a map centered around the mean latitude and longitude (this can be regarded as the centroid of the point pattern of Foursquare/Yelp venues)\n",
    "center_lat = examples['Latitude'].mean()\n",
    "center_lon = examples['Longitude'].mean()\n",
    "map_ = folium.Map(location=[center_lat, center_lon], zoom_start=6,control_scale=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1e2d5b-bcc0-41d0-8112-85326c6724b8",
   "metadata": {},
   "source": [
    "The first parameter `location` takes a pair of latitude, longitude values as an input (organized as a list) which will determine where the map will be positioned when users create the map. `zoom_start` parameter adjusts the default zoom-level for the map (the larger the number the closer the map is zoomed at). `control_scale` defines if the map should have a scale bar or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa4d7fa-3ac5-46ab-962e-63ad99bd4638",
   "metadata": {},
   "source": [
    "Let’s see what the map looks like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a0ddac-c709-481f-b202-da536afc58da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the map\n",
    "map_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c08e57-681c-4ca1-964a-f1c9b5f095d3",
   "metadata": {},
   "source": [
    "Next, we can start visualising our data using `Folium`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa03e188-4a5f-435c-9ff2-eedc87421b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a map centered around the mean latitude and longitude\n",
    "center_lat = examples['Latitude'].mean()\n",
    "center_lon = examples['Longitude'].mean()\n",
    "map_ = folium.Map(location=[center_lat, center_lon], zoom_start=6)\n",
    "\n",
    "# Add markers for each location in the data\n",
    "for index, row in examples.iterrows():\n",
    "    folium.Marker(\n",
    "        location=[row['Latitude'], row['Longitude']],\n",
    "        popup=f\"Category: {row['Category']}\"  # Use 'Category' for the popup label\n",
    "    ).add_to(map_)\n",
    "\n",
    "# Display the map\n",
    "map_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fcb0ce-e70b-4090-9f43-7b9ae446b391",
   "metadata": {},
   "source": [
    "That’s it! Now we have a cool interactive map with some markers on it, marking the distribution of our data and what category each point is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ce994f-bcb8-4108-b295-e3a503beed35",
   "metadata": {},
   "source": [
    "## 3 WordCloud to perform word frequency analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b554f5b6-3d14-4e79-97e2-38e5bd031b85",
   "metadata": {},
   "source": [
    "Now that we know the spatial distribution of local venues in Bristol, what else are you curious about?\n",
    "\n",
    "In addition to geographic information (coordinates), the interactive map also shows the category of each local venue. Then, one might be wondering which catrgory is most popular in my study area (e.g. Bristol in this practical)? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd69b14-556b-40e2-b083-37f6250ee675",
   "metadata": {},
   "source": [
    "To answer this question, we should compute the frequency of each category in our data. In this practical, we will use `WordCloud` to do it. `WordCould` draws a “cloud” of words, where the position and size of the word is determined by its frequency in the document. In Python, we can use the package `wordcloud` to implement it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9395e63-aa29-413c-b119-a0f0a9ff762b",
   "metadata": {},
   "source": [
    "However, `WordCloud` is a natural language processing (NLP) method; hence the input to this model must be a document contaning a collection of words. In order to adopt this technique, what is the document in our case study?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1ea236-6ca3-4f46-815e-7baf5f5d5a4e",
   "metadata": {},
   "source": [
    "There can be many ways and you are encouraged to be creative in answering this question. One way we will explore here is to regard the whole City of Bristol as a document. Its words/tokens are those local venues' categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bb6325-4e81-4870-a86a-df90a4731c96",
   "metadata": {},
   "source": [
    "First, let's concatenate all the text content from the ‘Category’ column in the ‘Foursquare_response’ dataset into a single long string: `text`. We will later regard this variable as a document.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd008fa7-5267-4bca-b85d-15256526f3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ' '.join(examples['Category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfeb2f8a",
   "metadata": {},
   "source": [
    "Next, we will directly call `WordCloud()` function in python. See comments below for details of using this function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7800429c-f6b2-484d-8087-2dae7aed2782",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "# Create a WordCloud object and generate the word cloud from the text\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "# width=800, height=400: Set the width and height of the word cloud image to 800x400 pixels\n",
    "# background_color='white': Set the background color of the word cloud image to white\n",
    "# generate(text): Generate the word cloud image from the concatenated text\n",
    "\n",
    "# Create a figure to display the word cloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "# figsize=(10, 5): Set the size of the figure to 10x5 inches\n",
    "\n",
    "# Display the word cloud image\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "# imshow(wordcloud): Display the word cloud image\n",
    "# interpolation='bilinear': Use bilinear interpolation to smooth the image\n",
    "\n",
    "# Hide the axes\n",
    "plt.axis('off')\n",
    "# axis('off'): Hide the axes for a cleaner look\n",
    "\n",
    "# Show the figure\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6aabe2-80c7-4707-9249-7346e53a40c4",
   "metadata": {},
   "source": [
    "### Group Discussion - Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f22781e-601b-4fb6-b8b5-161c3046ced1",
   "metadata": {},
   "source": [
    "Is this result correct? (Hint: check the data content displayed by `.head()` in the previous step again!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0fb7e5-a2f7-49ea-8815-e3161db4ec98",
   "metadata": {},
   "source": [
    "Actually, `wordcloud` library uses space as delimiters to distinguish words by default. Therefore, phrases like \"History Museum\" will be split into two separate words, \"History\" and \"Museum\". Therefore, we need to do a little preprocessing on this data set. See code below on how to implement it in Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226bd161-1406-447d-9f72-1022fc4706a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Replace the space in the Category column with underscores.\n",
    "modified_categories = examples['Category'].str.replace(' ', '_')\n",
    "\n",
    "# Join into a single string\n",
    "text = ' '.join(modified_categories)\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba6256a-67c0-4d81-bc46-25014a9fa283",
   "metadata": {},
   "source": [
    "## Task 1- Practicing WordCloud technique with Yelp data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e1e420-133a-4e77-9769-818544e85810",
   "metadata": {},
   "source": [
    "OK! It's your turn! Could you implement a word cloud using the Yelp data set? \n",
    "\n",
    "You only need to do two things: load the Yelp data, which is already cleaned and shared, and draw a word cloud based on its column `Review` using similar code as we've covered so far."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8adafd-fc72-47c4-af2a-253e48353966",
   "metadata": {},
   "source": [
    "Discuss within your team, which word(s) appear most frequently in Yelp's reviews? Any insights from the result?   Please upload your conclusion to our [Padlet](https://padlet.com/qingyacheng/submission-request/axjQb8J8eLYjXgYo)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e67a56-97ee-485f-a0bc-a38881db140b",
   "metadata": {},
   "source": [
    "## 4 Topic Modelling on Yelp's reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbade5c-3f0c-47fa-b046-526822707704",
   "metadata": {},
   "source": [
    "`WordCloud` visualizes key vocabularies and their frequencies in a document. By regarding a city (or a region) as a document and place categories as its words, this technique enables us to directly understand which place categories dominates the city (or region) the most, based on which we could estimate the city/region's major functionality. For instance, you could see \"Pub\" occurs most frequently in the dataset, so our study region might be a cluster of pubs. This technique is useful; however, simply using frequency might miss the meaning of these words. For instance, the term \"Pub\" might be similar to \"Bar\" in semantics. \n",
    "\n",
    "To fill this gap, topic moldeing is introduced in NLP. It performs deeper analysis of textual data by inferring the distribution of topics across each document, thereby revealing the (implicit) semantics underlying the text. Hence, it could capture more complex meanings hidden in texts, rather than simply relying on the frequency of words. It heavily involves probablistic models. Topic modeling is useful for the purpose of document clustering, information retrieval from unstructured text, feature selection, etc.\n",
    "\n",
    "There are many such topic modeling methods, including Latent Semantic Analysis or Indexing(LSA/LSI), Hierarchical Dirichlet process (HDP), and Latent Dirichlet Allocation(LDA). We will cover LDA in this session due to its popularity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4f8a72-83cd-4122-a6da-de7aacab4846",
   "metadata": {},
   "source": [
    "LDA considers each document as a collection of topics and each topic as collection of words/terms. You can see there is a hidden/middle layer - called topic - here between document and words. Once you set up the parameters (e.g., the number of topics), the model will then optimise the topic distribution of the document, as well as the word distribution of each topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238e7d99-4848-4e4b-a0df-1baa08b2ad5a",
   "metadata": {},
   "source": [
    "In this experiment, we will use the dataset `Yelp` and its column `Review` to introduce how LDA works. Note that it may not be a good dataset because it is relatively small, so the result may be difficult to interpret. You could later try the method on a larger dataset yourself. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d592135-9c66-45a7-815c-2e8ef507ad66",
   "metadata": {},
   "source": [
    "Make sure the following libraries are installed and imported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a16449-1e4b-476b-a43e-a893def86426",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a0364b-9c0f-4548-9624-3bbe2169c1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "Yelp_dataset=pd.read_csv('./Yelp_cleaned(new).csv')# make sure the directory is correct in your case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df453509-ad8c-4286-a58d-2b0e3f9ed573",
   "metadata": {},
   "source": [
    "Since LDA tries to model the semantic distribution of words/terms for document, with topics as the latent variable, we need to pre-process a bit of the text-based `Review` column. Namely, the data (text) must be converted to a matrix that can be processed by python. The matrix here is called Document-Term Matrix (DTM).   \n",
    "\n",
    "Document-Term Matrix (DTM) is a matrix that represents the frequency of terms in a document. Each row represents a document, each column represents a term, and each element in the matrix represents the frequency of a term in a document. This is a very intuitive way to convert text-based paragraphs/sentences into numbers.\n",
    "\n",
    "The library of `gensim` has a specific way of generating the Document-Term Matrix. First, we need to create a corpus, which is a collection of all the words/terms that have been used in the `Review` column. By doing so, we iterate each row of the `Review` column, convert all words into their lower cases, and tokenize the sentences. Tokenize here means to separate a full sentence into a collection of individual words/terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37db25f3-7fd4-4c29-8e82-5d0c54ffaa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "corpus = [word_tokenize(review.lower()) for review in Yelp_dataset['Review']]# Tokenize each review in the Yelp_dataset, converting all text to lowercase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee9321e",
   "metadata": {},
   "source": [
    "The resulted `corpus` is a list of list. The outer list is a collection of reiviews. Later, each review will be regarded as a document. The inner list is a collecton of words/terms that are used in the review.  Next, we need to convert this corpus to an object of `Dictionary` (defined by `gensim`). This `Dictionary` object is a mapping that includes all unique words/terms that appear in the `corpus`, each associated with a unique ID.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706eebd3-08af-491f-bc90-c311dc7b8549",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(corpus)#create a dictionary obejct using the corpus\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2ee981-72bc-4c1f-a48c-c8112c7ed993",
   "metadata": {},
   "source": [
    "Finally, we could generate the DTM object using `doc2bow()` function. `bow` here means “bag-of-words”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ef1940-626d-45b6-b92a-e77c5997203f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in corpus] #Create document-term matrix\n",
    "doc_term_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8d63fa-151c-4578-b62b-2ba461c645f2",
   "metadata": {},
   "source": [
    "The `doc2bow()` method converts documents into a bag-of-words representation. In this representation, each document is represented as a list containing word IDs and their frequencies. For example, a document might be represented as (word_id1, count1), (word_id2, count2), ..., where word_id is the ID of the word, and count is the number of times that word appears in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17acd18-bc1b-489a-9f44-a361081c2502",
   "metadata": {},
   "source": [
    "The resulted `doc_term_matrix` variable is a list of list. Specifically, the inner list represent a document/review, where the tuples indicate the index of the word and its frequency. For example, for the first document/review [(0, 1), (1, 1), (2, 1), ...], the 0th word is repeated once in the review, the 1st word repeated once, and the 2nd word repeated once, and so on. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783101c7-bcf1-4f48-9597-8ba37fe6ac7a",
   "metadata": {},
   "source": [
    "With such a Document-Term Matrix, we could then build a LDA model. One example of the code is shown as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de1f8f8-a7a7-4220-939d-f069069aaa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 2\n",
    "lda_model = LdaModel(\n",
    "    corpus=doc_term_matrix,\n",
    "    id2word=dictionary,\n",
    "    num_topics=num_topics,\n",
    "    random_state=0,\n",
    "    eval_every=None\n",
    ")\n",
    "#Adjust the above parameters and observe what happens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f147abc-fc6f-416f-9208-83c9ba87a488",
   "metadata": {},
   "source": [
    "To generate an LDA model, we input the `corpus`, represented as `doc_term_matrix` now, together with several parameters.\n",
    "\n",
    "`num_topics`: indicates the number of topics that will be used in the model\n",
    "\n",
    "`id2word`: is the vocabulary generated from the `corpora.Dictionary()` function\n",
    "\n",
    "`random_state`: used for reproducibility (because we need to randomly initialize some weights of the model, everytime you run the model, it might result into different outputs. Setting up a seed value here guarantees you that the models will be trained the same if input data and parameters are all the same)\n",
    "\n",
    "`eval_every`: Log perplexity (an evaluation metric for the model) is estimated every that many updates. If it is `None`, the model will only be evaluated at the end of the training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04313464-0d50-413b-ac8f-951fd7256138",
   "metadata": {},
   "source": [
    "After the model is built, we can then see how the topic(s) are represented:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9678c7b4-d151-49a6-86ed-6961309b6e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print topic words\n",
    "for topic_idx, topic in lda_model.show_topics(num_topics=num_topics, formatted=False):\n",
    "    print(f\"Topic {topic_idx}:\")\n",
    "    print(\" \".join([word for word, _ in topic]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e83576b",
   "metadata": {},
   "source": [
    "Another way of showing the topic (and its distribution):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3987e91-da24-43ee-8864-45bde3993b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fe9dcd-7445-47b0-9f33-3f36496023d4",
   "metadata": {},
   "source": [
    "As can be seen, the two topics (listed as 0 and 1) are reprsented as a distribution of words, and each of the 2781 unique words is given weights based on their importances in representing the topics. In other words, it implies which word is more likely to be used in the specific topic.\n",
    "\n",
    "For example, Topic 0 seems to be focused on positive reviews of restaurants in Bristol with words like \"good\", \"food\", and \"restrant\" being heavily used. In contrast, Topic 1 seems to be focused on a small pub or a similar place in Bristol, where reviewers expressed their enjoyment and satisfaction with the place."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485a82d2-93bb-4cb0-9a2c-80337067ed4c",
   "metadata": {},
   "source": [
    "With these topics and their representations, we can next check how each document (people's review on Yelp) is represented using the topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea7d7c5-7b87-419e-8c2d-1c2d306b4f6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count = 0 # used to indicate the index of document (review)\n",
    "for i in lda_model[doc_term_matrix]: # we iterate over the lda_model variable and i is the distribution of each doc in terms of the topics\n",
    "    print(\"doc: \", count, i)\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258ab55d-290f-4043-92b3-09f4b062f6b5",
   "metadata": {},
   "source": [
    "From the output, we see that each document now is represented as a distribution of topics. For example, for `doc: 0`, it is represented using topic 0, and its probability is 0.9515989. Since topic 0 dominates the distribution (with a probability of 0.9515989), we can then say `doc: 0` is more about \"pleasant experience at the restaurant, praising the food, service, and overall experience\" (this is interpreted based on the topic distribution you get from the last step)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c23a8d5",
   "metadata": {},
   "source": [
    "Note that we used a small dataset to illustrate topic modeling for urban analysis. In your future project/learning, you could apply it to a real world, bigger dataset, and you could use it creatvely too. The important take-away is that we could combine geographic data analysis with natural langurage processing to understand how human beings perceive the living environment and the world in large scale, thanks to social media data and rapid (Geo)AI advancement. This kind of approach is gaining tremendous popularity in (geographic) data science these days.     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c982e5-1c54-412f-9296-0cb89e5ff4ca",
   "metadata": {},
   "source": [
    "## Task 2- Adjusting LDA parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f390793-5c04-4606-92df-eba349d77347",
   "metadata": {},
   "source": [
    "Alright, it's time for you to practice! This time, you only need to modify the parameters of the LDA model and observe how changes in the parameters affect the results. Please don't limit yourself to the parameter settings provided in the sample code; you could refer to the [official documentation](https://radimrehurek.com/gensim/models/ldamodel.html) to explore additional parameters available for LDA and understand their functions and effects. Once again, please keep in mind that since this dataset is quite small, there might not be a \"best\" parameter setting, so feel free to experiment and discuss your creative results within your team, and share them on [Padlet](https://padlet.com/qingyacheng/submission-request/axjQb8J8eLYjXgYo)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24452921-2c66-4435-83ec-bc9c64c62c78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
